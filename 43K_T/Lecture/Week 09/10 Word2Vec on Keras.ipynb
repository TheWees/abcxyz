{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Word2Vec Implemented on Keras\n",
    "keras is a quite high-level deep learning library. In this notebook, we are going to implement two word2vec models: CBoW and Skip-gram. The utilized corpus is IMDB movie review dataset. http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. How to load pre-trained word vectors\n",
    "2. Reading in the IMDB Sentiment Dataset and Iterating over files in Python\n",
    "3. Build Skip-gram Model\n",
    "4. Build CBoW Model\n",
    "5. Memory-friendly Data Generation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load pre-trained word vectors\n",
    "\n",
    "- You can find the word2vec project here: https://code.google.com/archive/p/word2vec/\n",
    "- Download the word embeddings from the section **Pre-trained word and phrase vectors**. It is named `GoogleNews-vectors-negative300.bin.gz (3.4G)`\n",
    "- Use gensim that you can easily load these wordvectors and utilize their functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.05126953 -0.02233887 -0.17285156  0.16113281 -0.08447266  0.05737305\n",
      "  0.05859375 -0.08251953 -0.01538086 -0.06347656]\n",
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431607246399), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n",
      "cereal\n",
      "0.76640123\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "dog = model['dog']\n",
    "print(dog.shape)\n",
    "print(dog[:10])\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read in the IMDB Sentiment Dataset\n",
    "\n",
    "- You can access the imdb data folder in BT5153_data folder.\n",
    "- Each movie review is a text file and they are under two different folders: pos and neg.\n",
    "- We need to iterate over these files and load them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_dataset(imdb_path):\n",
    "    # imdb_path is the base path \n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    # contain two sub-folders named pos and neg\n",
    "    for cat in ['pos', 'neg']:\n",
    "        dset_path = os.path.join(imdb_path, cat)\n",
    "        # loop in each folder and get the file name for each txt.\n",
    "        for fname in sorted(os.listdir(dset_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dset_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read()) # load the data into memory\n",
    "                label = 0 if cat == 'neg' else 1\n",
    "                train_labels.append(label)\n",
    "    imdbdf = pd.DataFrame(\n",
    "             {'text': train_texts,\n",
    "              'label': train_labels}\n",
    "             )\n",
    "    # shuffle the whole dataset\n",
    "    imdbdf = imdbdf.sample(frac=1).reset_index(drop=True)\n",
    "    # Return the dataset in dataframe format\n",
    "    return imdbdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples shape : 25000\n"
     ]
    }
   ],
   "source": [
    "df_corpus = load_imdb_dataset('../BT5153_data/imdb')\n",
    "print ('Train samples shape :', df_corpus.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  This movie has great stars in their earlier ye...      1\n",
      "1  This is the biggest insult to TMNT ever. Fortu...      0\n",
      "2  I only saw it once. This happened in 1952, I w...      1\n",
      "3  Sure, for it's super imagery and awesome sound...      1\n",
      "4  It's very sly for all of the 60's look to the ...      1\n"
     ]
    }
   ],
   "source": [
    "# 1 denotes positive and 0 is negative\n",
    "print(df_corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "def clean_txt(raw_txt):\n",
    "    # Function to clean raw text\n",
    "    # 1. Remove HTML\n",
    "    raw_txt = BeautifulSoup(raw_txt, \"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_txt) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                                             \n",
    "    # \n",
    "    #\n",
    "    # 4. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus['text'] = df_corpus.text.apply(clean_txt)\n",
    "corpus = df_corpus.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# check the corpus type, which is a list of string\n",
    "print(type(corpus))\n",
    "print(type(corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tokenization from Keras\n",
    "\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# learn the vocab\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "this is the biggest insult to tmnt ever fortunantely officially venus does not exist in canon tmnt there will never be a female turtle this took away from the tragic tale of male unique mutants who will never have a family of their own once gone no more the biggest mistake was crossing over power rangers to tmnt with a horrible episode the turtle s voices were wrong and they all acted out of character they could have done such a better job better designs and animatronics and no venus don t bother with this people it s cringe worthy material the lip flap was slow and unnatural looking they totally disrespected shredder the main baddie some dragonlord dude was corny the turtles looked corny with things hanging off their bodies what s with the thing around raph s thigh the silly looking sculpted plastrons if they looked normal acted in character and got rid of venus got rid of the stupid kiddie cartoon sounds and better writing it could have been good\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus[1]))\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `fit_on_texts` function is trying to build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 6, 1, 1103, 2334, 5, 15685, 123, 46352, 8169, 14042, 125, 23, 1752, 8, 9697, 15685, 39, 78, 113, 28, 3, 652, 7366, 10, 551, 242, 37, 1, 1545, 768, 4, 887, 932, 8479, 35, 78, 113, 27, 3, 212, 4, 66, 203, 280, 809, 57, 51, 1, 1103, 1297, 13, 5971, 118, 647, 4771, 5, 15685, 16, 3, 516, 381, 1, 7366, 12, 2295, 69, 349, 2, 32, 30, 897, 44, 4, 103, 32, 96, 27, 220, 139, 3, 127, 288, 127, 4567, 2, 24102, 2, 57, 14042, 88, 20, 1397, 16, 10, 77, 7, 12, 4011, 1493, 802, 1, 5402, 26207, 13, 538, 2, 7466, 262, 32, 477, 26208, 19868, 1, 289, 8307, 48, 28758, 2643, 13, 2004, 1, 15099, 596, 2004, 16, 181, 2317, 122, 66, 2303, 47, 12, 16, 1, 152, 185, 37400, 12, 22375, 1, 694, 262, 32282, 46353, 45, 32, 596, 1226, 897, 8, 103, 2, 187, 3707, 4, 14042, 187, 3707, 4, 1, 371, 8961, 1050, 914, 2, 127, 478, 7, 96, 27, 76, 49]\n"
     ]
    }
   ],
   "source": [
    "# from string to a sequence of intergers\n",
    "# each word will be convereted to its vocab index\n",
    "seq_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "print(seq_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we are going to use a toy corpus instead of the IMDB corpus for a quick demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n",
      "4\n",
      "1\n",
      "2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# let us check the texts_to_sequences function\n",
    "toy_corpus = ['king is a strong man', \n",
    "              'queen is a wise woman', \n",
    "              'boy is a young man',\n",
    "              'girl is a young woman',\n",
    "              'prince is a young king',\n",
    "              'princess is a young queen',\n",
    "               'man is strong', \n",
    "               'woman is pretty',\n",
    "               'prince is a boy will be king',\n",
    "               'princess is a girl will be queen']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(toy_corpus)\n",
    "toy_seq_corpus = tokenizer.texts_to_sequences(toy_corpus)\n",
    "print(toy_seq_corpus[0])\n",
    "print(tokenizer.word_index['king'])\n",
    "print(tokenizer.word_index['is'])\n",
    "print(tokenizer.word_index['a'])\n",
    "print(tokenizer.word_index['strong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1fc3db1c7863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The KeyError means that the Tokenizer reserves 0 as an OOV words.\n",
    "- In practive, the first ebmedding in word embedding martix is for unkown words or chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Skip-gram Model\n",
    "\n",
    "- Here, we only use toy corpus for demo purpose.\n",
    "- Target: predict the nearby words based on the center word.\n",
    "<img src=\"word2vec-skip-gram.png\" alt=\"cbow\"\n",
    "\ttitle=\"cbow pic\" width=\"250\" height=\"150\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For skip-gram,  training data generation**:\n",
    "\n",
    "the input x is the center word index, the output x is one hot vector of the neary word index.\n",
    "For example, the toy corpus only contain two sentences.\n",
    "```\n",
    "I like apple \n",
    "I like reading books\n",
    "```\n",
    "1. The first step: build a vocab. which can be regarded as a mapping from words to interget index.\n",
    "\n",
    "Here, OOV-> 0, I -> 1, like -> 2, apple -> 3, reading -> 4, books -> 5.\n",
    "\n",
    "2. Then, we scan the corpus and creat the pair of center word and nearby word. Here, we set the window size is `one`.\n",
    "We have the following pair of input x and target y.\n",
    "\n",
    "<pre>\n",
    "words pair              numerical input       numerical output\n",
    "\n",
    "(I, like)                       1               [0,0,1,0,0,0]\n",
    "\n",
    "(like, I)                       2               [0,1,0,0,0,0]\n",
    "\n",
    "(like, apple)                   2               [0,0,0,1,0,0]\n",
    "\n",
    "(apple, like)                   4               [0,0,1,0,0,0]\n",
    "\n",
    "(I, like)\n",
    "\n",
    "(like, I)\n",
    "\n",
    "(like, reading)              \n",
    "\n",
    "(reading, like)\n",
    "\n",
    "(reading, books)\n",
    "\n",
    "(books, reading)                 5              [0,0,0,0,1,0]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the collection of lists of words index\n",
    "    window_size is the context size that defines 'nearby' words\n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    in_words   = [] \n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            for i in range(s, e):\n",
    "                if 0<= i < L and i != index:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(to_categorical(words[i], V))\n",
    "    return (in_words, labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus one is for OOV words\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 5\n",
    "window_size = 4\n",
    "ith = 0\n",
    "input_x, target_y =  generate_data(toy_seq_corpus, window_size, V)\n",
    "input_x           = np.array(input_x,dtype=np.int32)\n",
    "target_y          = np.array(target_y,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the first pair of input and output\n",
      "[4]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "check the third pair of input and output\n",
      "[4]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('check the first pair of input and output')\n",
    "print(input_x[0])\n",
    "print(target_y[0])\n",
    "print('check the third pair of input and output')\n",
    "print(input_x[2])\n",
    "print(target_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "print(toy_seq_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Config**\n",
    "\n",
    "It consists of two layers:\n",
    "\n",
    "1. The first layer is embeddings layer, which perform the lookup operation. Given the word index as the input, the layer output will return the corresponding vector\n",
    "\n",
    "2. The second layer is softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embeddings Layer**:\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "1. input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "2. output_dim: int >= 0. Dimension of the dense embedding.\n",
    "3. embeddings_initializer: Initializer for the embeddings matrix (see initializers).\n",
    "4. input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=17, output_dim=5, input_length=1, embeddings_initializer=\"glorot_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=5, activation=\"softmax\", units=17)`\n"
     ]
    }
   ],
   "source": [
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1, 5)              85        \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                102       \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85 = 17*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 2.8356\n",
      "Epoch 2/30\n",
      "204/204 [==============================] - 0s 206us/step - loss: 2.8190\n",
      "Epoch 3/30\n",
      "204/204 [==============================] - 0s 203us/step - loss: 2.8035\n",
      "Epoch 4/30\n",
      "204/204 [==============================] - 0s 182us/step - loss: 2.7885\n",
      "Epoch 5/30\n",
      "204/204 [==============================] - 0s 200us/step - loss: 2.7745\n",
      "Epoch 6/30\n",
      "204/204 [==============================] - 0s 212us/step - loss: 2.7610\n",
      "Epoch 7/30\n",
      "204/204 [==============================] - 0s 181us/step - loss: 2.7480\n",
      "Epoch 8/30\n",
      "204/204 [==============================] - 0s 196us/step - loss: 2.7357\n",
      "Epoch 9/30\n",
      "204/204 [==============================] - 0s 188us/step - loss: 2.7243\n",
      "Epoch 10/30\n",
      "204/204 [==============================] - 0s 223us/step - loss: 2.7130\n",
      "Epoch 11/30\n",
      "204/204 [==============================] - 0s 221us/step - loss: 2.7020\n",
      "Epoch 12/30\n",
      "204/204 [==============================] - 0s 186us/step - loss: 2.6914\n",
      "Epoch 13/30\n",
      "204/204 [==============================] - 0s 223us/step - loss: 2.6817\n",
      "Epoch 14/30\n",
      "204/204 [==============================] - 0s 191us/step - loss: 2.6722\n",
      "Epoch 15/30\n",
      "204/204 [==============================] - 0s 209us/step - loss: 2.6633\n",
      "Epoch 16/30\n",
      "204/204 [==============================] - 0s 211us/step - loss: 2.6547\n",
      "Epoch 17/30\n",
      "204/204 [==============================] - 0s 170us/step - loss: 2.6464\n",
      "Epoch 18/30\n",
      "204/204 [==============================] - 0s 179us/step - loss: 2.6388\n",
      "Epoch 19/30\n",
      "204/204 [==============================] - 0s 186us/step - loss: 2.6311\n",
      "Epoch 20/30\n",
      "204/204 [==============================] - 0s 196us/step - loss: 2.6237\n",
      "Epoch 21/30\n",
      "204/204 [==============================] - 0s 194us/step - loss: 2.6166\n",
      "Epoch 22/30\n",
      "204/204 [==============================] - 0s 204us/step - loss: 2.6098\n",
      "Epoch 23/30\n",
      "204/204 [==============================] - 0s 220us/step - loss: 2.6033\n",
      "Epoch 24/30\n",
      "204/204 [==============================] - 0s 198us/step - loss: 2.5971\n",
      "Epoch 25/30\n",
      "204/204 [==============================] - 0s 219us/step - loss: 2.5913\n",
      "Epoch 26/30\n",
      "204/204 [==============================] - 0s 193us/step - loss: 2.5856\n",
      "Epoch 27/30\n",
      "204/204 [==============================] - 0s 190us/step - loss: 2.5804\n",
      "Epoch 28/30\n",
      "204/204 [==============================] - 0s 189us/step - loss: 2.5751\n",
      "Epoch 29/30\n",
      "204/204 [==============================] - 0s 190us/step - loss: 2.5703\n",
      "Epoch 30/30\n",
      "204/204 [==============================] - 0s 180us/step - loss: 2.5658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af3525208>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.fit(input_x, target_y, batch_size=8, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How to save the learned word vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **the saved format for word vectors**\n",
    "<img src=\"saved_format.jpg\" alt=\"cbow\"\n",
    "\ttitle=\"saved format\" width=\"550\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab = 16, window = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **we can use gensim**\n",
    "\n",
    "Gensim is a production-ready open-source library for NLP problems.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strong', 0.9319900870323181),\n",
       " ('boy', 0.8408138751983643),\n",
       " ('is', 0.6083444356918335),\n",
       " ('queen', 0.5916460752487183),\n",
       " ('will', 0.44901859760284424),\n",
       " ('girl', 0.4101567566394806),\n",
       " ('wise', 0.3264765441417694),\n",
       " ('young', 0.2976924777030945),\n",
       " ('pretty', 0.2894490361213684),\n",
       " ('a', 0.22375084459781647)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build CBoW Model\n",
    "\n",
    "- CBoW's target is the prediction of center word.\n",
    "<img src=\"word2vec-cbow.png\" alt=\"cbow\"\n",
    "\ttitle=\"cbow pic\" width=\"250\" height=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For cbow,  training data generation**:\n",
    "\n",
    "the input x is the list of  context word index, the output x is one hot vector of the center word.\n",
    "For example, the toy corpus only contain two sentences.\n",
    "```\n",
    "I like apple \n",
    "I like reading books\n",
    "```\n",
    "1. The first step: build a vocab. which can be regarded as a mapping from words to interget index. \n",
    "Here OOV->0, I -> 1, like -> 2, apple -> 3, reading -> 4 books -> 5.\n",
    "\n",
    "2. Then, we scan the corpus and creat the pair of list of nearby word and center word. Here, we set the window size is `one`.\n",
    "We have the following pair of input x and target y.\n",
    "\n",
    "<pre>\n",
    "words pair                     numerical input       numerical output\n",
    "\n",
    "([like], I)                        [2]                 [0,1,0,0,0,0]\n",
    "\n",
    "([I, apple], like)                 [1,3]               [0,0,1,0,0,0]\n",
    "\n",
    "([like], apple)                    [2]                 [0,0,0,1,0,0]\n",
    "\n",
    "([like], I)                        [2]                 [0,1,0,0,0,0]\n",
    " \n",
    "([I, reading], like)               [1,4]               [0,1,0,0,0,0]\n",
    "\n",
    "([like, books], reading)           [2,5]               [0,0,0,0,1,0]\n",
    "\n",
    "([reading], books)                 [4]                 [0,0,0,0,0,1]\n",
    "</pre>\n",
    "\n",
    "3. At last, sometimes, we can not get the input context with enough length. For example, the first pair's numerical input only has one word index insetad of two. What we can do here is padding the short input so that all input data have the same length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prepare the training and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "def generate_data(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the list of sequence of words index\n",
    "    window_size is used to define  \n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    context_words   = []\n",
    "    center_words    = []\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)           \n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = to_categorical(labels, V)\n",
    "            context_words.append(x)\n",
    "            center_words.append(y)\n",
    "    return context_words, center_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ith = 0\n",
    "input_x, target_y = generate_data(toy_seq_corpus, window_size, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1, 8)\n",
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "input_x = np.array(input_x)\n",
    "print(input_x.shape)\n",
    "input_x = np.squeeze(input_x)  # sequeeze the second dimesion as on\n",
    "print(input_x.shape)\n",
    "target_y = np.array(target_y)\n",
    "target_y = np.squeeze(target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 2 8 5]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(input_x[0])\n",
    "print(target_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "print(toy_seq_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lambda Layer**:\n",
    "\n",
    "Wraps arbitrary expression as a Layer object.\n",
    "    1. function: The function to be evaluated. Takes input tensor as first argument. usually based on backend\n",
    "    2. output_shape: Expected output shape from function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "# sum all embeddings \n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "## add softmax layer\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 8, 5)              85        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                102       \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 2.8328\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 331us/step - loss: 2.8290\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 354us/step - loss: 2.8256\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 312us/step - loss: 2.8225\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 340us/step - loss: 2.8191\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 400us/step - loss: 2.8153\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 334us/step - loss: 2.8120\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 341us/step - loss: 2.8084\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 350us/step - loss: 2.8052\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 473us/step - loss: 2.8013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af34f5748>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# Train the model, iterating on the data in batches of 512 samples\n",
    "cbow.fit(input_x, target_y, batch_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Memory-friendly Data Generation\n",
    "\n",
    "- Here, we modify the data generation function of skip-gram\n",
    "- `yield`: it will return generators. And generators do not store all the values in memory. It will return value during each iteration.\n",
    "\n",
    "sample code\n",
    "```\n",
    "generator = (x * x for x in range(3))\n",
    "for i in generator:\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_live(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the list of sequence of words index\n",
    "    window_size is used to define  \n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        labels   = []\n",
    "        in_words = [] \n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            for i in range(s, e):\n",
    "                if 0<= i < L and i != index:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "        x = np.array(in_words,dtype=np.int32)\n",
    "        y = to_categorical(labels, V)\n",
    "        yield (x, y)# return a generators, doesn't store all the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25.75001883506775\n",
      "1 25.72290015220642\n",
      "2 25.696035861968994\n",
      "3 25.669301748275757\n",
      "4 25.642654418945312\n",
      "5 25.61606216430664\n",
      "6 25.589520692825317\n",
      "7 25.563032627105713\n",
      "8 25.53661823272705\n",
      "9 25.510297298431396\n",
      "10 25.484103202819824\n",
      "11 25.458067655563354\n",
      "12 25.43221688270569\n",
      "13 25.406575918197632\n",
      "14 25.38117027282715\n",
      "15 25.356019258499146\n",
      "16 25.33113932609558\n",
      "17 25.30654764175415\n",
      "18 25.28225612640381\n",
      "19 25.258276224136353\n",
      "20 25.23461675643921\n",
      "21 25.211283445358276\n",
      "22 25.18828320503235\n",
      "23 25.16561770439148\n",
      "24 25.143290042877197\n",
      "25 25.12130117416382\n",
      "26 25.09965229034424\n",
      "27 25.07834005355835\n",
      "28 25.057363033294678\n",
      "29 25.036719799041748\n",
      "30 25.016404151916504\n",
      "31 24.996421575546265\n",
      "32 24.976744651794434\n",
      "33 24.957393884658813\n",
      "34 24.938352584838867\n",
      "35 24.919610023498535\n",
      "36 24.90116810798645\n",
      "37 24.88301706314087\n",
      "38 24.865150690078735\n",
      "39 24.84756088256836\n",
      "40 24.830241680145264\n",
      "41 24.813185691833496\n",
      "42 24.796387672424316\n",
      "43 24.779839038848877\n",
      "44 24.763533353805542\n",
      "45 24.747464418411255\n",
      "46 24.731623888015747\n",
      "47 24.716007947921753\n",
      "48 24.700609922409058\n",
      "49 24.685423135757446\n"
     ]
    }
   ],
   "source": [
    "## here you should define skipgram from scratch\n",
    "\n",
    "for ite in range(50):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_live(toy_seq_corpus, window_size, V):\n",
    "        #updated parameters based on data samples provided without regard to any fixed batch size\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
